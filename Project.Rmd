---
title: "STAT 221 Final Project - Mammoth Snow Depth"
author: "John Rapp Farnes | 405461225"
date: "3/19 2020"
header-includes:
   - \usepackage{float}
output:
  pdf_document:
    toc: true
    number_section: true
---

\newpage

```{r setup, include = F}
knitr::opts_chunk$set(echo = F, fig.pos = '!htb')
```

# Introduction

## Background

Mammoth Mountain is situated in Northern California and is know for its great alpine ski and snowboarding conditions. For this purpose, the mountain features a ski resort with the same name. This resort has has more than 3,500 acres of ski-able terrain and is serviced by 28 lifts, recieving over 1 million annual visitors. For Southern Californian residents especially, the mountain is of interest as it is one of the closest high-quality ski resorts, about a 4-6 hour drive away from Los Angeles. \cite{mammoth}

Most people familiar with alpine sports consider snow depth one of the most important conditions for the sport, as this affects which runs are open and how "good" the skiing is considered. As decisions about travelling to a ski resort generally are done in advance and require some planning (e.g. booking a cabin), being able to predicts future conditions would be useful to the alpine skiier. In addition, the entire ski economy of a mountain such as Mammoth, including the resort, workers as well as restaurants and stores in the sorrounding city, face uncertainty over how many visitors the mountain will get a given week or year as this drives revenue. As visitorships likely is correlated with snow depth, forecasting it would be an important tool also for those stake holders. This paper will attempt to do that: **model the snow depth at Mammoth mountain in order to make predictions on future skiing conditions**.

## Data set
Data on historic snow depth at Mammoth were obtained from the reporting of Mammoth Mountain Ski Area, through a third party website \cite{mammoth_data}. The website does not provide the data easily in a downloadable format, hence the data was obtained through injecting JavaScript into a browser client that extracted the data from the browser's JavaScript enviroment and printed it in a JSON format. This raw data is shown in figure \ref{fig:raw_data}, featuring 1791 recordings from 2011-12-01 through 2020-03-02 of daily snow depth measured in inches. Upon looking at the graph, two issues with the raw data are found: First, the dates in the off-season (outside of the winter months) are not included, rather the years are concatinated together in a single time-frame. Second, some values within the recorded period are missing and reported as 0. Hence, the data needed to be further processed and cleaned before fitting any models on it.

## Cleaning data
The first step in cleaning the data was to include the missing dates in order to capture the full time-frame of the series. The next step was handling the missing values, both in the off-season as well as the missing recorded values. In order to deal with the missing recorded values, as well as to make lower variance predictions on snow depth further in the future than a couple of days, the data was aggregated and averaged (disregarding the missing values) per week, resulting in a weekly time series. This week was defined as starting a Saturday, as this a day of interest for many weekend skieers. The off-season missing values were replaced by 0s, as this is an accurate description of the snow depth during those months -- there rarely is any snow on Mammoth during the summer. The resulting data after cleaning is shown in figure \ref{fig:cleaned}, featuring 431 weeks. The availability of data per month is displayed in figure \ref{fig:availability}, showing that data exists for the most part Dec-May, with less than 50% Jun-Oct, again reaching ~80% in November.

# Analyis
Looking at the graph in figure \ref{fig:cleaned}, it is clear that the ski season of 2020 has a far lower snow depth than prior years, an unfortunate fact for skiiers this season. In order to study other properties than this obvious observation, time-series methods were applied.

## Series properties
Figure \ref{fig:acf_series} plots the ACF and PACF of the series. The ACF shows periodic behaviour with a 1 year period that appears to be slowly tailing off, while the PACF has 2 (barely) significant values and then cuts off, followed by a significant value at lag 1 year. This implies that a seasonal AR model may be a good description of the series. The 1 year period is easily seen also in the periodogram shown in figure \ref{fig:period_series}, together with a 4 year period, both being significant peaks. The 4 year period may be an artifact of the data being recorded for 8 years and these years having a pattern of yearly depth by random chance.

The data does not appear to be stationary as it has an obvious yearly trend and a clear pattern of snowing and thawing in the beginning and end of seasons. As such, the data must first be detrended before ARIMA models can be applied.

## Detrending
The most obvious trend in the data that may be removed is the 1 year seasonal trend. This trend can be seen in figure \ref{fig:avg_weekly} which shows the average snow depth during the different weeks of the year (week 1 through week 53), which has a clear sinusoidal shape. As such, a first attempt at detrending the data was made by subracting the average snow depth of the week of year to every data point. The result of this may be seen in figure \ref{fig:detrended_week}. Looking at the graph, it does however not appear to be stationary. One artifact of this detrending method is that the different years experience different levels of snow, causing years with less snow to have a clear negative valley and years with more snow to have a positive peak. Looking at the snow depth by week of the year for all years simoultaneaosly, seen  in figure \ref{fig:weekly}, makes it clear that the snow level is very different each year and that the average hence is not a good predictor.

One way to mitigate this fact is to study values in relation to the peak snow level of the year. The assumption one makes is that the dynamics that determine what the peak snow depth, or how high the snow peak will be a given year, is different from the dynamics that govern how its snowing and thawing during the year, dividing the analysis into a macro and micro model. With this approach, one model could be used to predict peak snow depth a given year, e.g. assuming that they are independent and follow a certain distribution. Under this assumption, predicting the snow depth for the rest of a season  may be done by only looking at how the current and past values have been in relation to the peak snow depth during the year, which is the approach implemented in this paper. Figure \ref{fig:detrended_relative} shows the snow depth relative to the peak value of the season, from this the average of the week of the year of every data point is subtracted in order to remove seasonality. This model assumes that the dynamics of when and how much it snows is similar from year to year. The resulting detrended graph is shown in figure \ref{fig:detrended_before_linear} both with the off-seasons as 0s, as well as with with missing values ommited, all ski seasons shown next to each other. This series finally looks fairly stationary, while still having some features that can modelled with time-series methods. As the series however appears to still have an upwards linear trend, the final detrended data is created by removing this linear trend, shown in figure \ref{fig:detrended}.

As for the missing values in the data set, there are multiple ways to handle them, the most obvious being either setting them to 0 or removing those dates from the data. As the length of the seasons differ from year to year, removing the off-seasons from the data makes seasonality measures such as the periodogram harder to interpret, and seasonal components less accurate. On the other hand, including the 0s makes the variance in the data vary significantly as it is 0 in the off-season, which is not consistent with the ARIMA assumptions.

## Detrended series properties
Figure \ref{fig:acf_detrended} shows the ACF and PACF of the detrended series. The ACF is tailing of faster for this series, however still with a peak after one year, implying that the yearly feature has not been fully taken out. The PACF only has a single (barely) significant value then cuts off. This implies that a low order AR model may be appropriate, again potentially with a yearly seasonal component. The periodogram, seen in figure \ref{fig:period_detrended}, has 5 significant peaks (in blue), with period ~2.1, ~0.69, ~0.44, ~0.28 and ~0.20 years. Figure \ref{fig:period_detrended_peaks} shows that the peaks has a clear pattern, the first having frequency ~0.48, and then being fairly evenly spaced. This means that the detrended series still has periodic components to it, however the frequencies are not easily interpretable as to what they describe. Figure \ref{fig:period_detrended_ar} plots the parametric periodogram, showing the periodogram of the fitting the best fitting AR(p) model. For the detrended series, an AR(2) fit the best, having no clear peaks but a low frequency spectrum. These two periodograms both tell us is that the noise in the data is mostly low frequency, and therefore fairly smooth.

## Model fitting
In order to fit a (seasonal) ARIMA model to the detrended data, different models with different values of $p$, $q$ and $d$ as well as seasonal components were compared and evaluated based on their AIC-value, starting with bigger more complex models and lowering the complexity as long as terms are insignificant. This process yielded an SARIMA(2,0,1)(0,0,1)[52] model as the optimal model choice. This yearly seasonal model fitting better than a simpler ARMA(2,0,1) implies that there is still some seasonality in the data after detrending. In mathematical terms, this SARIMA(2,0,1)(0,0,1)[52] model can be written as:

$$
\begin{aligned}
  \phi(B)x_t &= \theta (B)\Theta (B^{52})w_t \implies \\
  (1 - \phi_1 B - \phi_2 B^2)x_t &= (1+\theta B)(1+\Theta B^{52})w_t \implies \\
  x_{t} - \phi_1 x_{t-1} - \phi_2 x_{t-2} &= (1+\Theta B^{52}+\theta B +\theta \Theta B^{53})w_{t} \implies \\
  x_{t} &= w_{t} + \phi_1 x_{t-1} + \phi_2 x_{t-2}+\theta w_{t-1} +\Theta w_{t-52} +\theta \Theta w_{t-53}
\end{aligned}
$$
For the fitted optimal model we have coefficients shown in table \ref{tab:coeff}, all being significant.

\begin{table}[h]
\centering
\begin{tabular}{lllll}
         & $\phi_1$ & $\phi_2$ & $\theta$ & $\Theta$ \\
Estimate & 1.4321   & -0.5351  & -0.3756  & 0.1261   \\
S.E.     & 0.1595   & 0.1381   & 0.1766   & 0.0549  
\end{tabular}
\caption{\label{tab:coeff}The coefficients of the fitted optimal SARIMA(2,0,1)(0,0,1)[52] model}
\end{table}

## Model interpretation

The low order of the AR and MA components indicated that the process has a "short memory", i.e. that the snow depth a given week primarily depends on the snow depths the two weeks prior, as well as the snow depth last year at that time. The $\phi_1$ coefficient being positive $> 1$, while the $\phi_2$ being negative can be interpreted as the momentum of the previous week being preserved, while the momentum of the week before counteracts it. Additionally, the $\phi_1$ and $\phi_2$ coefficient adding to one can be interpreted as the snow being a weighted average of the last two weeks. The $\theta$ coefficient being negative implies that shocks, e.g. snow falls/thaws one week will have the opposite effect the previous week. $\Theta$ being positve and fairly small means that snow depth across years are connected and positively corrolated.

## Model evaluation
The performance and goodness of fit of the model can be evaluated in two main ways: studying the residuals to see how well the data conforms to the assumptions of the model, and looking at predictions of the model.


### Residual analysis
In figure \ref{fig:resid_plots_missing}, several plots showing different characteristics of the resisduals are show: the standardized residuals, the ACF of residuals, a normal Q-Q plot and p-values for the Ljung-Box statistic, for the detrended data with missing values set to 0. Looking at the standardized residuals in the plot, it is clear that the residuals does not fully fullfil the assumptions, as many have an absolut value greater than 2, and multiple even greater than 4 which would improbable if they were normally distributed. There also seems to be some pattern left in them. The ACF has no significant peaks and hence there should be no clear dependence between residuals, which is confirmed by the p-values of the Ljung-Box statistic, where only one dips below the significance level. The normal Q-Q plot shows that the rediduals does not appear to be normal, as many are diverging from the normal quantile line.

One reason for the standardized residuals diverging so far from the normal assumptions is that the data, and with it the residuals, contains a lot of 0s which can be seen in figure \ref{fig:hist_resid_missing}, showing a histogram of the residuals compared to a sampled normal distribution. These zereoes lower the variance estimate, making the standardized residuals bigger. One way to mitigate this problem is to train the model on a data set exluding the missing values, concatinating the seasons together. The residual statistics of this model is shown in figure \ref{fig:resid_plots}. For this model, the standardized residuals are still bigger than expected by the normal assumption but to a lesser degree. The ACF has one significant value, but this could be a statistical fluke. The Ljung-Box looks better and the QQ plot similar to the previous model. In figure \ref{fig:hist_resid} plotting a histogram of the absolute values of the residuals compared to a samples normal distribution, the residuals more closely resembles a normal distribution than the model with missing values included. In the histogram, it is clear that the residuals have more "low" values as well as "high" values and fewer in-between, compared to the normal distribution. This could reflect the two dynamics seen in the data in figure \ref{fig:cleaned}, where it snows and thaws agressively in the beginning and end of the season, with smaller differences within the season.

Looking at the periodogram of the residuals, shown in figure \ref{fig:period_resid}, it has some peaks however they do not appear significant. Further, a parametric periodogram results in an AR(0) model, shown in figure \ref{fig:period_resid_ar}. Hence, the periodogram resembles that of white noise, implying that the model captures the features in the data and that there are few features left in the residuals.

In conclusion, the model does not completely meet the assumptions of the model, but is close to doing so. As a result, it should make fairly accurate predictions, however the uncertainty is likely bigger than predicted by the model.

### Prediction
Using the trained model, predictions were made for the coming 30 weeks, results (after undoing the detrending) shown in figure \ref{fig:forecast}, and predicts that the snow depth will increase, which doesn't seem consistent with the trajectory of the current season at first glance. However, looking at this time of year previous years (shown in dashed gray lines), the snow level tends to increase after this time of year. After the peak, the snow depth is expected to decrease quickly, consistent with the dynamics of the ski season. The standard errors are fairly small, implying a confident prediction, however as previously discussed this may not actually be the case.

Figure \ref{fig:forecast_back} shows the prediction of the model looking back 10 weeks, i.e. how it would predict this time of year if we were looking at data from earlier in the season, with the actual outcome shown in a dashed line. The model does not accurately predict the outcome of the season, as it predicts it going down rather than up. However the actual observation mostly fits within the two standard deviation area.

# Conclusion
In this paper, time series methods were used to analyze and predict the snow depth at Mammoth mountain. An issue with the data-set that had to be handled in order to successfully apply the methods on it was missing values, primarily in the off-season but also to some extent within the season. This issue was dealt with by aggregating and averageing the data by week, as well as setting missing values to 0, assuming there was no snow in the off-season. Next, the data was detrended by normalizing the depth according to the yearly peak, as well as removing a yearly trend. This approach assumes a macro model which predicts the peak snow depth a given year, as well as a micro model which models the snowing and thawing dynamics during the season. The micro model is implemented in this paper to predict the snow levels of the current year. The detrended data appears stationary, implying a SARIMA model may be appropriate. An optimal model was choosing based on the optimizing for lowest AIC, resulting in a SARIMA(2,0,1)(0,0,1)[52] model. The model performed best when fed with missing values removed from the data set, rather than set to 0. The resulting model appears to fairly well describe the dynamics of the data, based on the residual and forecast analysis. The model predicts that the snow depth will increase the coming weeks, good news for skiiers this season. However, as always, the model should be used as an indication rather than the truth, as its results are uncertain.

Next steps could be to try other models on the data to more accurately represent it. As can be seen in plots (figure \ref{fig:cleaned}) of the data, the variance of the snow-depth varies significantly during the year: being 0 in the off-season, followed by fast snowing and thawing in the beginning and end of seasons and lower variance in between. This problem is partially mitigated by the detrending process, as this feature is less prevailant in the detrended data (as seen in \ref{fig:detrended}), however the detrended data also shows this pattern as indicated by the histogram of the residuals showin in figure \ref{fig:hist_resid}. A model that has a variable variance, e.g. GARCH model could hence be more appropriate. In order to forecast the snow depth of next year, with this approach, a macro model would also have to be developed. A primitive such would be to assume that the peak level follows e.g. a i.i.d normal distribution with the sample mean and variance of the recorded history. As there are only 8 years on the recorded history the variance of this estimate would proabibly very high however. Additionally, the histogram of the peaks shown in figure \ref{fig:hist_peak} does not appear to be normally distributed.

\newpage

# Appendix - Figures and graphs

```{r include=F}
library(astsa)
library(imputeTS)
library(rjson)
library(forecast)
library(tseries)
library(pracma)
```

```{r fig.cap="\\label{fig:raw_data}Raw Mammoth snow depth data", fig.height=4, fig.pos="!htb"}
# Read and plot raw data
data <- fromJSON(file = "./mammoth.json")
plot(data$depth, type="o", main="Raw data", ylab="Snow depth (in)")
```

```{r, include=F}
#Fill in missing days
data$dates <- as.Date(data$dates, "%Y-%m-%d")

all_dates <- seq(data$dates[1], data$dates[length(data$dates)], by = "day")

length(data$dates) / length(all_dates)

all_depths <- rep(NA, length(all_dates))

for (date in intersect(data$dates, all_dates)) {
  all_depths[which(all_dates == date)] <- data$depth[which(data$dates == date)[1]]
}
```

```{r, include=F}
table(as.numeric(format.Date(data$date, "%m")), data$depth == 0)

all_depths[which(all_depths == 0)] <- NA
```

```{r, include=F}
data <- data.frame(date = all_dates, depth = all_depths)

nrow(data)

plot(data, type="o")
```

```{r, include=F}
(tab <- table(as.numeric(format.Date(data$date, "%m")), is.na(data$depth)))

plot(tab[, 1] / (tab[, 1] + tab[, 2]), type="o")
```

```{r, include=F}

mean_ignore_na <- function(x) {
  non_na <- x[which(!is.na(x))]
  return(ifelse(length(non_na) == 0, NA, mean(non_na)))
}

first_sat <- 3
last_sat <- length(data$date) - 2

week_length <- 7
nr_weeks <- (last_sat - first_sat) / week_length

week_start <- data$date[first_sat]
week_end <- data$date[last_sat]

weekly_depths <- mean_ignore_na(data$depth[1:first_sat])

# for(i in seq(from = first_sat, to = last_sat, by = week_length)) {
for (i in 2:(nr_weeks + 1)) {
  sunday <- (first_sat + 1) + (i - 2)*week_length
  saturday <- sunday + 6

  weekly_depths[i] <- mean_ignore_na(data$depth[sunday:saturday])
}

weeks <- seq(data$date[first_sat], data$date[last_sat], by=7)

data <- data.frame(
  depth = weekly_depths,
  date = weeks
)

plot(data$depth, type="o")
```

```{r, include=F}
data$depth[which(is.na(data$depth))] <- 0
```


```{r fig.cap="\\label{fig:cleaned}Mammoth snow depth data after initial cleaning", fig.height=4}
series <- ts(
  data$depth,
  start=c(as.numeric(format(data$date[1], "%Y")), as.numeric(format(data$date[1], "%U"))),
  # end=c(as.numeric(format(data$date[length(data$date)], "%Y")), as.numeric(format(data$date[length(data$date)], "%U"))),
  frequency=365.25/7
)
plot(series, main="Cleaned data", ylab="Snow depth (in)")
```


```{r fig.cap="\\label{fig:availability}Percent of weeks where data is available, by month", fig.height=4}
tab <- table(as.numeric(format.Date(data$date, "%m")), data$depth == 0)

month.labels <- c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D")
barplot(100*tab[, 1] / (tab[, 1] + tab[, 2]), names.arg = month.labels, ylab="% available data", main="Available data by month")
```


```{r, fig.cap="\\label{fig:acf_series}ACF and PACF for the cleaned time-series", fig.height=7}
par(mfrow=c(2,1))
plot(acf(series, lag.max = 52 * 2, plot = F), ylim=c(-1,1), xlim=c(0.1,2), main="acf")
plot(pacf(series, lag.max = 52 * 2, plot = F), ylim=c(-1,1), xlim=c(0.1,2), main="pacf")
```

```{r, include=F}
plot(diff(series), main="First difference")
abline(v=time(series)[which(abs(diff(series))>50)], col="blue")

plot(diff(series, differences = 2))
acf(diff(series), lag.max = 52 * 2)
pacf(diff(series), lag.max = 52 * 2)

adf.test(series)
```

```{r, fig.cap="\\label{fig:period_series}Periodogram for the cleaned data, peeks at period 1 and 4 years shown with a 95% significance interval", fig.height=7}
par(mfrow=c(2,1))
pg <- mvspec(series, log="no")

k = kernel("daniell", c(1,1))

pg <- mvspec(series, kernel=k, log="no")

l <- 1/sum(k$coef^2)

alpha <- 0.05
U = qchisq(alpha/2, 2*l)
L = qchisq(1-alpha/2, 2*l)

peaks <- findpeaks(pg$spec, npeaks = 2)[,2]

# 1/pg$freq[peaks]

conf <- list(l = 2*l*pg$spec[peaks]/L, u = 2*l*pg$spec[peaks]/U)
  
segments(x0=pg$freq[peaks],y0=conf$l,x1=pg$freq[peaks],y1=conf$u,col="blue")
segments(x0=pg$freq[peaks]-0.1,y0=conf$l,x1=pg$freq[peaks]+0.1,y1=conf$l,col="blue")

# plot(pg$freq[peaks])
```



```{r, fig.cap="\\label{fig:avg_weekly}Average snow depth by week of the year", fig.height=4}

periods <- as.numeric(format(data$date, "%V"))
# periods <- as.numeric(format(data$date, "%V"))
# periods <- as.numeric(format(data$date, "%m"))

agg <- aggregate(
  data$depth,
  by=list(period = periods),
  FUN=mean
)
plot(1:range(periods)[2], agg$x, main="Weekly average snow depth", type="l", ylab="Snow depth (in)", xlab="Week of year")
```


```{r, fig.cap="\\label{fig:detrended_week}Snow depth detrended by subtracting average of week of the year", fig.height=4}
detrended <- data$depth - agg$x[match(periods, agg$period)]

# plot(detrended, type="l")
# lines(detrended, type="l", col="red")
plot(detrended, main="Cleaned data", type="l", ylab="Snow depth (in)")

```


```{r, fig.cap="\\label{fig:weekly}Weekly snow depth of all years", fig.height=4}
plot(periods, data$depth, ylab="Snow depth (in)", xlab="Week of year")
```


```{r, include=F}

seasonal_offset <- 10*6

agg_max_season <- aggregate(
  data$depth,
  by=list(period = as.numeric(format(data$date + seasonal_offset, "%Y"))),
  FUN=max
)


plot(data$date, data$depth, type="l")
points(as.Date(paste(agg_max_season$period,"/1/1",sep=""), format="%Y/%m/%d"), agg_max_season$x)

```


```{r, fig.cap="\\label{fig:detrended_relative}First step of detrending the data, taking the values relative to the peak of the season", fig.height=4}
normalized_depth <- data$depth/agg_max_season$x[match(as.numeric(format(data$date + seasonal_offset, "%Y")), agg_max_season$period)]

plot(data$date, normalized_depth, type="l", ylab="Relative snow depth (%)", xlab="Time")

```


```{r, fig.cap="\\label{fig:detrended_before_linear}Detrended data after subracting the yearly trend", fig.height=7}
weeks <- as.numeric(format(data$date, "%V"))

agg_mean_week <- aggregate(
  normalized_depth,
  by=list(period = weeks),
  FUN=mean
)
# plot(agg_mean_week$period, agg_mean_week$x)

# plot(weeks, normalized_depth)

detrended <- normalized_depth - agg_mean_week$x[match(weeks, agg_mean_week$period)]
# detrended <- detrended - mean(detrended)

par(mfrow=c(2,1))
plot(data$date, detrended, type="l", ylab="Relative snow depth (%)", xlab="Time")

detrended_collapsed <- detrended[which(data$depth != 0)]
plot(detrended_collapsed, type="l", ylab="Relative snow depth (%)")

```

```{r, include=F}
regr <-list(time=time(detrended))

lin.mod <- lm(detrended ~ time, data=regr)
summary(lin.mod)

# plot(detrended, type="l")
# lines(predict(lin.mod, regr), type="l", col="red")
```



```{r, fig.cap="\\label{fig:detrended}Final detrended data", fig.height=7}

detrended <- ts(
  detrended,
  start=start(series),
  end=end(series),
  frequency=frequency(series)
)
par(mfrow=c(2,1))
plot(detrended, main="Detrended data", ylab="Relative snow depth (%)", xlab="Time")

detrended_collapsed <- detrended[which(data$depth != 0)]
plot(detrended_collapsed, type="l", ylab="Relative snow depth (%)")
```


```{r, include=F}
plot(diff(detrended), main="First difference")
abline(v=time(series)[which(abs(diff(series))>50)], col="blue")

plot(diff(detrended, differences = 2))
acf(diff(detrended), lag.max = 52 * 2)
pacf(diff(detrended), lag.max = 52 * 2)

adf.test(detrended)
adf.test(diff(detrended))
```

```{r, fig.cap="\\label{fig:acf_detrended}ACF and PACF for the detrended time-series", fig.height=7}
par(mfrow=c(2,1))
plot(acf(detrended, lag.max = 52 * 2, plot = F), ylim=c(-1,1), xlim=c(0.1,2), main="acf")
plot(pacf(detrended, lag.max = 52 * 2, plot = F), ylim=c(-1,1), xlim=c(0.1,2), main="pacf")
```


```{r, fig.cap="\\label{fig:period_detrended}Periodogram for the detrended data, peeks shown with a 95% significance interval, significant peaks marked in blue", fig.height=7}
par(mfrow=c(2,1))
pg <- mvspec(detrended, log="no")

k = kernel("daniell", c(1,1))
# pg <- mvspec(diff(detrended), kernel=k, log="no")
pg <- mvspec(detrended, kernel=k, log="no")

l <- 1/sum(k$coef^2)

alpha <- 0.05
U = qchisq(alpha/2, 2*l)
L = qchisq(1-alpha/2, 2*l)

peaks <- findpeaks(pg$spec, npeaks = 8)[,2]
# points(pg$freq[peaks], pg$spec[peaks])

# 1/pg$freq[peaks]

conf <- list(l = 2*l*pg$spec[peaks]/L, u = 2*l*pg$spec[peaks]/U)

segments(x0=pg$freq[peaks[1:5]],y0=conf$l[1:5],x1=pg$freq[peaks[1:5]],y1=conf$u[1:5],col="blue")
segments(x0=pg$freq[peaks[1:5]]-0.1,y0=conf$l[1:5],x1=pg$freq[peaks[1:5]]+0.1,y1=conf$l[1:5],col="blue")

segments(x0=pg$freq[peaks[6:8]],y0=conf$l[6:8],x1=pg$freq[peaks[6:8]],y1=conf$u[6:8],col="gray")
segments(x0=pg$freq[peaks[6:8]]-0.1,y0=conf$l[6:8],x1=pg$freq[peaks[6:8]]+0.1,y1=conf$l[6:8],col="gray")

```


```{r, fig.cap="\\label{fig:period_detrended_peaks}Frequencies of the significant peaks in the periodogram of the detrended data", fig.height=4}
plot(pg$freq[peaks[1:5]], type="o", xlab="Peak number", ylab="Peak frequency")
```



```{r, fig.cap="\\label{fig:period_detrended_ar}Parametric periodogram of the detrended data, showing the periodogram of an AR(2)", fig.height=4}
pg <- spec.ar(detrended)
```

```{r, include=F}
plot.resids <- function(fitit, xdata) {
    S <- 1
    layout(matrix(c(1, 2, 4, 1, 3, 4), ncol = 2))
    par(mar = c(2.2, 2, 1, 0.25) + 0.5, mgp = c(1.6, 0.6, 
        0))
    rs <- fitit$residuals
    stdres <- rs/sqrt(fitit$sigma2)
    num <- sum(!is.na(rs))
    plot.ts(stdres, main = "Standardized Residuals", ylab = "")
    alag <- max(10 + sqrt(num), 3 * S)
    ACF = stats::acf(rs, alag, plot = FALSE, na.action = na.pass)$acf[-1]
    LAG = 1:alag/frequency(xdata)
    L = 2/sqrt(num)
    plot(LAG, ACF, type = "h", ylim = c(min(ACF) - 0.1, min(1, 
        max(ACF + 0.4))), main = "ACF of Residuals")
    abline(h = c(0, -L, L), lty = c(1, 2, 2), col = c(1, 
        4, 4))
    stats::qqnorm(stdres, main = "Normal Q-Q Plot of Std Residuals")
    sR <- !is.na(stdres)
    ord <- order(stdres[sR])
    ord.stdres <- stdres[sR][ord]
    PP <- stats::ppoints(num)
    z <- stats::qnorm(PP)
    y <- stats::quantile(ord.stdres, c(0.25, 0.75), names = FALSE, 
        type = 7, na.rm = TRUE)
    x <- stats::qnorm(c(0.25, 0.75))
    b <- diff(y)/diff(x)
    a <- y[1L] - b * x[1L]
    abline(a, b, col = 4)
    SE <- (b/dnorm(z)) * sqrt(PP * (1 - PP)/num)
    qqfit <- a + b * z
    U <- qqfit + 3.9 * SE
    L <- qqfit - 3.9 * SE
    z[1] = z[1] - 0.1
    z[length(z)] = z[length(z)] + 0.1
    xx <- c(z, rev(z))
    yy <- c(L, rev(U))
    polygon(xx, yy, border = NA, col = gray(0.6, alpha = 0.2))
    nlag <- ifelse(S < 7, 20, 3 * S)
    p <- length(fitit$model$phi)
    q <- length(fitit$model$theta)
    P <- 0
    Q <- 0
        ppq <- p + q + P + Q
    if (nlag < ppq + 8) {
        nlag = ppq + 8
    }
    pval <- numeric(nlag)
    for (i in (ppq + 1):nlag) {
        u <- stats::Box.test(rs, i, type = "Ljung-Box")$statistic
        pval[i] <- stats::pchisq(u, i - ppq, lower.tail = FALSE)
    }
    plot((ppq + 1):nlag, pval[(ppq + 1):nlag], xlab = "LAG (H)", 
        ylab = "p value", ylim = c(-0.1, 1), main = "p values for Ljung-Box statistic")
    abline(h = 0.05, lty = 2, col = "blue")
}

```

```{r, fig.cap="\\label{fig:resid_plots_missing} Plots of different characteristics of the best model's residuals fitted on the detrended data with missing values included", fig.height=7, message=FALSE}
# auto.arima(detrended)

model <- sarima(detrended, 2,0,1, 0,0,1, 1, details = F)


plot.resids(model$fit, detrended)

```

```{r, fig.cap="\\label{fig:hist_resid_missing} Histogram of residuals with model fitted on detrended data set including missing data, compared to a sampled normal distribution with same mean and standard deviation", fig.height=4}
best_model <- model$fit
set.seed(2020)
sample <- rnorm(length(best_model$residuals), mean(best_model$residuals), sqrt(std_err(best_model$residuals)))

p1 <- hist(abs(sample-mean(sample)), plot = F, seq(0, 0.4, by=0.025/2))
p2 <- hist(abs(best_model$residuals-mean(best_model$residuals)), plot = F, seq(0, 0.4, by=0.025/2))

plot(p2, col=rgb(0,0,1,1/4), xlim = c(0,0.4), main="Model residuals vs normal comparison", xlab="|x|")
plot(p1, col=rgb(1,0,0,1/4), xlim = c(0,0.4), add=T)

legend("topright", legend=c("Model residuals", "Sampled normal distribution"),fill=c(rgb(0,0,1,1/4), rgb(1,0,0,1/4)), cex=0.8)

```

```{r, fig.cap="\\label{fig:resid_plots} Plots of different characteristics of the best model's residuals fitted on the detrended data with missing values excluded", fig.height=7, message=FALSE, warning=F}
# auto.arima(detrended)

detrended <- ts(
  detrended[which(series != 0)],
  start=start(series),
  # end=end(series),
  frequency=frequency(series)
)


model <- sarima(detrended, 2,0,1, 0,0,1, 1, details = F)


plot.resids(model$fit, detrended)

```


```{r, include=F}
models <- list(
  arima(detrended, order=c(2,0,1), seasonal=list(order=c(0,0,1), period=52))#,
  # arima(detrended, order=c(2,0,2), seasonal=list(order=c(0,0,1), period=52)),
  
  # arima(detrended, order=c(2,0,1)),
  # arima(detrended, order=c(2,0,2))#,
)

models

lapply(models, function(x) {
  return(x$aic)
})

best_model <- models[[1]]
```

```{r, fig.cap="\\label{fig:hist_resid} Histogram of residuals with model fitted on detrended data set excluding missing data, compared to a sampled normal distribution with same mean and standard deviation", fig.height=4}
set.seed(2020)
sample <- rnorm(length(best_model$residuals), mean(best_model$residuals), sqrt(std_err(best_model$residuals)))

p1 <- hist(abs(sample-mean(sample)), plot = F, seq(0, 0.4, by=0.025))
p2 <- hist(abs(best_model$residuals-mean(best_model$residuals)), plot = F, seq(0, 0.4, by=0.025))

plot(p2, col=rgb(0,0,1,1/4), xlim = c(0,0.4), main="Model residuals vs normal comparison", xlab="|x|")
plot(p1, col=rgb(1,0,0,1/4), xlim = c(0,0.4), add=T)

legend("topright", legend=c("Model residuals", "Sampled normal distribution"),fill=c(rgb(0,0,1,1/4), rgb(1,0,0,1/4)), cex=0.8)

```



```{r, fig.cap="\\label{fig:period_resid}Periodogram for the residuals data, peeks shown with a 95% significance interval", fig.height=7}
# plot(best_model$residuals)
# abline(v=time(series)[which(abs(diff(series))>50)], col="blue")

par(mfrow=c(2,1))
pg <- mvspec(best_model$residuals, log="no")

k = kernel("daniell", c(2,2))
# pg <- mvspec(diff(detrended), kernel=k, log="no")
pg <- mvspec(best_model$residuals, kernel=k, log="no")

l <- 1/sum(k$coef^2)

alpha <- 0.05
U = qchisq(alpha/2, 2*l)
L = qchisq(1-alpha/2, 2*l)

peaks <- findpeaks(pg$spec, npeaks = 20, threshold = 0.00002)[,2]
# points(pg$freq[peaks], pg$spec[peaks])

# 1/pg$freq[peaks]

conf <- list(l = 2*l*pg$spec[peaks]/L, u = 2*l*pg$spec[peaks]/U)

significance_level <- 0.000165
peaks_siginificant <- pg$spec[peaks] > significance_level

segments(x0=pg$freq[peaks],y0=conf$l,x1=pg$freq[peaks],y1=conf$u,col=ifelse(peaks_siginificant, "blue", "gray"))
segments(x0=pg$freq[peaks]-0.1,y0=conf$l,x1=pg$freq[peaks]+0.1,y1=conf$l,col=ifelse(peaks_siginificant, "blue", "gray"))
```



```{r, fig.cap="\\label{fig:period_resid_ar}Parametric periodogram of the residuals, showing the periodogram of an AR(0)", fig.height=4}
pg <- spec.ar(best_model$residuals)
```


```{r, fig.cap="\\label{fig:forecast}Forecast of the snow depth the coming 30 weeks, the same time of year as the forecast start of every year shown in dashed vertical lines. One and two stardard deviations marked in gray.", fig.height=7}
par(mfrow=c(2,1))
dates <- data$date[series != 0]
s_dates <- dates
weeks <- as.numeric(format(dates, "%V"))

lintrended <- detrended + predict(lin.mod, list(time=regr$time[series!=0]))
normalized_depth <- lintrended + agg_mean_week$x[match(weeks, agg_mean_week$period)]
trended <- normalized_depth*agg_max_season$x[match(as.numeric(format(dates + seasonal_offset, "%Y")), agg_max_season$period)]

weeks_ahead <- 30
fore <- predict(best_model, n.ahead = weeks_ahead)

last_time <- time(detrended)[length(time(detrended))]

last_date <- data$date[length(data$date)]

freq <- frequency(series)

times <- seq(from=last_time + 1/freq, by = 1/freq, length.out = weeks_ahead)

dates <- seq(from=last_date + 1, by = 7, length.out = weeks_ahead)
weeks <- as.numeric(format(dates, "%V"))

pred <- ts(
  fore$pred,
  start=times[1],
  end=times[length(times)],
  frequency = frequency(series)
)

pred <- pred + predict(lin.mod, list(time=seq(from = regr$time[length(regr$time)], length.out=weeks_ahead)))
pred <- pred + agg_mean_week$x[match(weeks, agg_mean_week$period)]
pred <- pred*agg_max_season$x[match(as.numeric(format(dates + seasonal_offset, "%Y")), agg_max_season$period)]

se <- ts(
  fore$se,
  start=times[1],
  end=times[length(times)],
  frequency = frequency(series)
)
se <- se*agg_max_season$x[match(as.numeric(format(dates + seasonal_offset, "%Y")), agg_max_season$period)]


U = pred+se; L = pred-se
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))

UU = pred+se * 2; LL = pred-se * 2
xxx = c(time(UU), rev(time(UU))); yyy = c(LL, rev(UU))

plot(trended, ylab="Snow depth (in)", main="Snow depth forecast", xlab="Weeks in future", xaxt="n", xlim=c(2016.501, 2017.784), ylim=c(0,120))
axis(1, at=c(times[seq(from=1,to=length(times),by=5)],times[length(times)]), labels=c(seq(from=1,to=length(times),by=5), length(times)))

lines(pred, type="o", col=2)
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
polygon(xxx, yyy, border = 8, col = gray(.6, alpha = .2))

plot(trended, ylab="Snow depth (in)", main="Snow depth forecast", xlab="Weeks in future", xaxt="n", xlim=c(2011.901, 2017.784))
axis(1, at=c(times[seq(from=1,to=length(times),by=15)],times[length(times)]), labels=c(seq(from=1,to=length(times),by=15), length(times)))
lines(pred, type="o", col=2)
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
polygon(xxx, yyy, border = 8, col = gray(.6, alpha = .2))


for (t in 0:8) {
  abline(v=time(detrended)[which(s_dates==last_date - 52*7*t)], lt="dashed", col="gray")
}
```



```{r, fig.cap="\\label{fig:forecast_back}Forecast of the snow depth the coming 20 weeks, looking back 10 weeks. Actual data shown dashed, one and two stardard deviations marked in gray.", fig.height=4}
weeks_omit <- 10
ommitted_series <- subset(detrended, start=1, end=length(detrended) - weeks_omit)
best_model <- arima(ommitted_series, order=c(2,0,1), seasonal=list(order=c(0,0,1), period=52))


weeks_ahead <- 30 - weeks_omit
fore <- predict(best_model, n.ahead = weeks_ahead)

last_time <- time(detrended)[length(time(detrended)) - weeks_omit]

last_date <- data$date[length(data$date) - weeks_omit]

freq <- frequency(series)

times <- seq(from=last_time + 1/freq, by = 1/freq, length.out = weeks_ahead)
dates <- seq(from=last_date + 1, by = 7, length.out = weeks_ahead)
weeks <- as.numeric(format(dates, "%V"))

pred <- ts(
  fore$pred,
  start=times[1],
  end=times[length(times)],
  frequency = frequency(series)
)

pred <- pred + predict(lin.mod, list(time=seq(from = regr$time[length(regr$time)], length.out=weeks_ahead)))
pred <- pred + agg_mean_week$x[match(weeks, agg_mean_week$period)]
pred <- pred*agg_max_season$x[match(as.numeric(format(dates + seasonal_offset, "%Y")), agg_max_season$period)]

se <- ts(
  fore$se,
  start=times[1],
  end=times[length(times)],
  frequency = frequency(series)
)
se <- se*agg_max_season$x[match(as.numeric(format(dates + seasonal_offset, "%Y")), agg_max_season$period)]


U = pred+se; L = pred-se
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))

UU = pred+se * 2; LL = pred-se * 2
xxx = c(time(UU), rev(time(UU))); yyy = c(LL, rev(UU))

plot(subset(trended, start=1, end=length(detrended) - weeks_omit), ylab="Snow depth (in)", xaxt="n", main="Snow depth forecast", xlim=c(2016.301, 2017.684), ylim=c(0,120))
axis(1, at=c(times[seq(from=1,to=length(times),by=5)],times[length(times)]), labels=c(seq(from=1,to=length(times),by=5), length(times)))

lines(pred, type="o", col=2)
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
polygon(xxx, yyy, border = 8, col = gray(.6, alpha = .2))

lines(trended, lt="dashed",col=1)

```


```{r, fig.cap="\\label{fig:hist_peak}Histogram of the peak snow depths in the recorded history of Mammoth", fig.height=4}
hist(agg_max_season$x, xlab = "Snow depth (in)", main="Histogram of peak snow depth")
```

\newpage

\begin{thebibliography}{9}

\bibitem{mammoth} 
Mammoth Mountain,
\\\texttt{https://www.mammothmountain.com/ }

\bibitem{mammoth_data} 
Mammoth Mountain, On The Snow
\\\texttt{https://www.onthesnow.com/california/mammoth-mountain-ski-area/historical-snowfall.html }

\end{thebibliography}